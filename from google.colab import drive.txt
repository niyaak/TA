from google.colab import drive
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from sklearn.metrics import precision_score, recall_score, f1_score
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate
from tensorflow.keras.models import Model
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Mount Google Drive
drive.mount('/content/drive')

# Verifikasi GPU
print("Available GPU:", tf.config.list_physical_devices('GPU'))

# Define paths and parameters
base_dir = '/content/drive/MyDrive/datasetld3kelas'
train_dir = os.path.join(base_dir, 'train')
val_dir = os.path.join(base_dir, 'valid')

img_height, img_width = 288, 512
batch_size = 16
learning_rate = 0.0001
epochs = 50
early_stop_patience = 5

# Class weights
class_weights = {0: 1.0, 1: 1.5, 2: 1.0, 3: 1.0}
loss = SparseCategoricalCrossentropy()

# Function to load class mapping
def load_classes(folder):
    classes_file = os.path.join(folder, '_classes.csv')
    if not os.path.exists(classes_file):
        raise FileNotFoundError(f"Classes file not found in {folder}")
    
    print(f"Loading classes from {classes_file}...")
    df = pd.read_csv(classes_file)
    pixel_col = [col for col in df.columns if 'Pixel' in col][0]
    class_col = [col for col in df.columns if 'Class' in col][0]
    class_mapping = {int(row[pixel_col]): idx for idx, row in df.iterrows()}
    print(f"Loaded class mapping: {class_mapping}")
    return class_mapping

# Function to load and preprocess data
def load_data(folder):
    try:
        class_mapping = load_classes(folder)
    except FileNotFoundError as e:
        print(e)
        return None, None, None

    images, masks = [], []
    for file_name in os.listdir(folder):
        if file_name.endswith('.jpg'):
            img_path = os.path.join(folder, file_name)
            mask_path = os.path.join(folder, file_name.replace('.jpg', '_mask.png'))

            if not os.path.exists(mask_path):
                print(f"Warning: Mask file not found for {file_name}")
                continue

            # Load and normalize the image
            img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_height, img_width))
            img = tf.keras.preprocessing.image.img_to_array(img) / 255.0
            images.append(img)

            # Load the mask
            mask = tf.keras.preprocessing.image.load_img(mask_path, target_size=(img_height, img_width), color_mode='grayscale')
            mask = tf.keras.preprocessing.image.img_to_array(mask).astype(np.uint8).squeeze()

            for pixel_value, class_idx in class_mapping.items():
                mask[mask == pixel_value] = class_idx
            masks.append(mask)

    if not images or not masks:
        raise ValueError("No valid data found in folder")

    images = np.array(images)
    masks = np.array(masks)
    masks = np.expand_dims(masks, axis=-1)
    return images, masks, len(class_mapping)

# Load training and validation data
print("Loading training data...")
train_images, train_masks, num_classes_train = load_data(train_dir)
print("Loading validation data...")
val_images, val_masks, num_classes_val = load_data(val_dir)

if num_classes_train != num_classes_val:
    raise ValueError("Number of classes in training and validation datasets are not the same.")

num_classes = num_classes_train

# Define U-Net model
def unet_model(input_size=(img_height, img_width, 3), num_classes=num_classes):
    inputs = Input(input_size)

    # Encoder
    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)
    p1 = MaxPooling2D((2, 2))(c1)

    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)
    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)
    p2 = MaxPooling2D((2, 2))(c2)

    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)
    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)
    p3 = MaxPooling2D((2, 2))(c3)

    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)
    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)
    p4 = MaxPooling2D((2, 2))(c4)

    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)
    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)

    u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)
    u6 = concatenate([u6, c4])
    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)
    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)

    u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)
    u7 = concatenate([u7, c3])
    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)
    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)

    u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)
    u8 = concatenate([u8, c2])
    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)
    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)

    u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)
    u9 = concatenate([u9, c1])
    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)
    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)

    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(c9)

    return Model(inputs, outputs)

# Compile model
model = unet_model()
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
              loss=loss,
              metrics=['accuracy'])

# Callbacks
checkpoint = ModelCheckpoint('/content/drive/MyDrive/dataset3kelasrev/best_model.keras', monitor='val_accuracy', save_best_only=True, mode='max')
early_stopping = EarlyStopping(monitor='val_accuracy', patience=early_stop_patience, mode='max', verbose=1)

# Train the model
print("Training the model...")
history = model.fit(train_images, train_masks,
                    validation_data=(val_images, val_masks),
                    epochs=epochs,
                    batch_size=batch_size,
                    callbacks=[checkpoint, early_stopping])

# Evaluate precision, recall, and F1-score
val_preds = model.predict(val_images, batch_size=batch_size)
val_preds = np.argmax(val_preds, axis=-1)

def calculate_metrics(y_true, y_pred):
    y_true = y_true.flatten()
    y_pred = y_pred.flatten()
    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
    return precision, recall, f1

precision, recall, f1 = calculate_metrics(val_masks.flatten(), val_preds.flatten())
print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}")

# Visualize sample prediction
plt.subplot(1, 3, 1)
plt.imshow(val_images[0])
plt.title("Input Image")

plt.subplot(1, 3, 2)
plt.imshow(val_masks[0].squeeze(), cmap='gray')
plt.title("Ground Truth")

plt.subplot(1, 3, 3)
plt.imshow(val_preds[0], cmap='gray')
plt.title("Prediction")

plt.show()